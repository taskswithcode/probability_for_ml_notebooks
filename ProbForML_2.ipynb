{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProbForML_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taskswithcode/probability_for_ml_notebooks/blob/main/ProbForML_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a notebook link for the video [What is the loss function used in language models like ChatGPT?](https://youtu.be/LOh5-LTdosU)"
      ],
      "metadata": {
        "id": "wutkB5DK8uIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To recap, entropy assigns a number,  to the information content in an event.\n",
        "\n",
        "- High probability events have low surprise value (so low entropy).\n",
        "- Low probability events have high surprise value (so high entropy).\n",
        "- Numerically, **the entropy of an event** is the **negative logarithm of its probability**.\n",
        "\n",
        "The **entropy of probability distribution** is the sum of the entropy of the individual events **weighted** by the probability of those events.\n",
        "\n",
        "The **goal of this notebook** is to illustrate the underlying reason for **why  entropy can be used as a loss function**\n",
        "\n",
        "- Imagine a model is learning to predict the distribution for some input. That is, the model predicts, 80% cat and 20% dog, for the image above.\n",
        "\n",
        "- Lets use  these predictions to compute the entropy of each event, and then weight each event by the true probability of those events,  say  90% cat and 10% dog for the image above.\n",
        "\n",
        "- This value we computed, also called **cross entropy**, will be lowest only if, the predicted probabilities are the same as the true probabilities, which is not, in this case.\n",
        "\n",
        "- That is, **cross entropy is lowest, only when the predicted distribution, is the same as the true distribution for an input**  - which is 90% cat and 10% in our example.\n",
        "\n",
        "- So **cross entropy** - **which is a single number**, captures **how far off**, the models **predicted distribution is**, from the **true distribution**.\n",
        "\n",
        "- Note that when we  compute the entropy of a probability distribution,  each event is weighted by the probability of that event.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UZA-CqITV1UN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of a True and Predicted probability distribution *(from video)*\n"
      ],
      "metadata": {
        "id": "Tx02cE-HC6il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [0.9, 0.1] #True distribution"
      ],
      "metadata": {
        "id": "GbbcmyzxDEzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = [0.8, 0.2] #Predicted distribution"
      ],
      "metadata": {
        "id": "fdH4Cy1QJWFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.  Cross entropy function"
      ],
      "metadata": {
        "id": "2PVhRppdQmdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cross_entropy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the cross entropy between a true and a predicted distribution\n",
        "\n",
        "    Args:\n",
        "    y_true: A list representing the true probability distribution.\n",
        "    y_pred: A list representing the predicted probability distribution.\n",
        "\n",
        "    Returns:\n",
        "    The cross entropy of the two distributions.\n",
        "    \"\"\"\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # The distributions should be valid probability distributions\n",
        "    assert np.isclose(np.sum(y_true), 1), \"True distribution should sum to 1.\"\n",
        "    assert np.isclose(np.sum(y_pred), 1), \"Predicted distribution should sum to 1.\"\n",
        "    assert (y_true >= 0).all(), \"All elements in the true distribution should be non-negative.\"\n",
        "    assert (y_pred >= 0).all(), \"All elements in the predicted distribution should be non-negative.\"\n",
        "\n",
        "    # Exclude zero values to avoid log(0)\n",
        "    mask = y_true > 0\n",
        "    y_true = y_true[mask]\n",
        "    y_pred = y_pred[mask]\n",
        "\n",
        "    return round(-np.sum(y_true * np.log2(y_pred)),3) #Cross entropy computation"
      ],
      "metadata": {
        "id": "4ZEz5mlaDpxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Compute Cross entropy"
      ],
      "metadata": {
        "id": "f5QgvnGeIikv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### First compute cross entropy when the predicted distribution is the same as true distribution. In this case cross entropy value is the same as the entropy of the true distribution"
      ],
      "metadata": {
        "id": "36WPGnYvg4nM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_true)\n",
        "print(cross_entropy(y_true, y_true)) #Note this is the lowest value we can get for the entropy of the distribution [.9,.1]\n",
        "                                    #this is the case where the cross entropy value is the same as the entropy of the true distribution"
      ],
      "metadata": {
        "id": "QcDhSZfisi46",
        "outputId": "79af71ce-c8f5-4417-8f3a-a788e088aefd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9, 0.1]\n",
            "0.469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Next compute the cross entropy for cases where predicted distribution is not the same as the true distribution"
      ],
      "metadata": {
        "id": "1bcCLI27hHfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,9):\n",
        "    i = round(i*.1,1)\n",
        "    y_pred = [round(y_true[0] - i,1), round(y_true[1] + i,1)]\n",
        "    print(y_pred)\n",
        "    print(cross_entropy(y_true, y_pred)) #All the cross entropy values will be more than the number computed above. So minimizing cross entropy loss enables a model to learn"
      ],
      "metadata": {
        "id": "yd8Pohhm910b",
        "outputId": "fc7e99f4-8066-426d-ba1c-82c0843cf0b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8, 0.2]\n",
            "0.522\n",
            "[0.7, 0.3]\n",
            "0.637\n",
            "[0.6, 0.4]\n",
            "0.795\n",
            "[0.5, 0.5]\n",
            "1.0\n",
            "[0.4, 0.6]\n",
            "1.263\n",
            "[0.3, 0.7]\n",
            "1.615\n",
            "[0.2, 0.8]\n",
            "2.122\n",
            "[0.1, 0.9]\n",
            "3.005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note all the values of cross entropy computed above is larger than the entropy of the true distribution. This fact makes cross entropy useful as a loss function*\n",
        "\n",
        "Since the cross entropy value is always greater than entropy of the true distribution, the difference between these two values is always positive. This difference is called **KL-divergence.** and is also used as a loss function in some machine learning tasks. Cross entropy loss is typically used for supervised classification tasks. KL divergence is used in models like Variational autoencoders to quantify the difference between a latent variable distribution and a prior distribution, like a Gaussian."
      ],
      "metadata": {
        "id": "C9k8MwALhQnB"
      }
    }
  ]
}